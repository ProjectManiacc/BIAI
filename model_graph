digraph {
	graph [size="136.35,136.35"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1961225538352 [label="
 (1, 4, 480, 480)" fillcolor=darkolivegreen1]
	1962223845232 [label=UpsampleBilinear2DBackward0]
	1962223844944 -> 1962223845232
	1962223844944 [label=ConvolutionBackward0]
	1962223844080 -> 1962223844944
	1962223844080 [label=AddBackward0]
	1961845644832 -> 1962223844080
	1961845644832 [label=AddBackward0]
	1961845643248 -> 1961845644832
	1961845643248 [label=AddBackward0]
	1961845644976 -> 1961845643248
	1961845644976 [label=AddBackward0]
	1961845642768 -> 1961845644976
	1961845642768 [label=UpsampleBilinear2DBackward0]
	1961845644064 -> 1961845642768
	1961845644064 [label=ReluBackward0]
	1961845644112 -> 1961845644064
	1961845644112 [label=NativeGroupNormBackward0]
	1961845643824 -> 1961845644112
	1961845643824 [label=ConvolutionBackward0]
	1961845643104 -> 1961845643824
	1961845643104 [label=UpsampleBilinear2DBackward0]
	1961845643584 -> 1961845643104
	1961845643584 [label=ReluBackward0]
	1961845643392 -> 1961845643584
	1961845643392 [label=NativeGroupNormBackward0]
	1961845643488 -> 1961845643392
	1961845643488 [label=ConvolutionBackward0]
	1961845642960 -> 1961845643488
	1961845642960 [label=UpsampleBilinear2DBackward0]
	1961845644352 -> 1961845642960
	1961845644352 [label=ReluBackward0]
	1961845643008 -> 1961845644352
	1961845643008 [label=NativeGroupNormBackward0]
	1961974111056 -> 1961845643008
	1961974111056 [label=ConvolutionBackward0]
	1961974111728 -> 1961974111056
	1961974111728 [label=ConvolutionBackward0]
	1961974111776 -> 1961974111728
	1961974111776 [label=ReluBackward0]
	1961974110768 -> 1961974111776
	1961974110768 [label=AddBackward0]
	1961974108704 -> 1961974110768
	1961974108704 [label=NativeBatchNormBackward0]
	1961974110864 -> 1961974108704
	1961974110864 [label=ConvolutionBackward0]
	1961974078096 -> 1961974110864
	1961974078096 [label=ReluBackward0]
	1961974077760 -> 1961974078096
	1961974077760 [label=NativeBatchNormBackward0]
	1961974077568 -> 1961974077760
	1961974077568 [label=ConvolutionBackward0]
	1961974111872 -> 1961974077568
	1961974111872 [label=ReluBackward0]
	1961974077088 -> 1961974111872
	1961974077088 [label=AddBackward0]
	1961974076896 -> 1961974077088
	1961974076896 [label=NativeBatchNormBackward0]
	1961974076656 -> 1961974076896
	1961974076656 [label=ConvolutionBackward0]
	1961974076464 -> 1961974076656
	1961974076464 [label=ReluBackward0]
	1961974076320 -> 1961974076464
	1961974076320 [label=NativeBatchNormBackward0]
	1961974076128 -> 1961974076320
	1961974076128 [label=ConvolutionBackward0]
	1961974077040 -> 1961974076128
	1961974077040 [label=ReluBackward0]
	1961974075648 -> 1961974077040
	1961974075648 [label=AddBackward0]
	1961974075456 -> 1961974075648
	1961974075456 [label=NativeBatchNormBackward0]
	1961974050672 -> 1961974075456
	1961974050672 [label=ConvolutionBackward0]
	1961974050480 -> 1961974050672
	1961974050480 [label=ReluBackward0]
	1961974050240 -> 1961974050480
	1961974050240 [label=NativeBatchNormBackward0]
	1961974050048 -> 1961974050240
	1961974050048 [label=ConvolutionBackward0]
	1961974049760 -> 1961974050048
	1961974049760 [label=ReluBackward0]
	1961974049520 -> 1961974049760
	1961974049520 [label=AddBackward0]
	1961974049424 -> 1961974049520
	1961974049424 [label=NativeBatchNormBackward0]
	1961974049280 -> 1961974049424
	1961974049280 [label=ConvolutionBackward0]
	1961974048992 -> 1961974049280
	1961974048992 [label=ReluBackward0]
	1961974048752 -> 1961974048992
	1961974048752 [label=NativeBatchNormBackward0]
	1961974048656 -> 1961974048752
	1961974048656 [label=ConvolutionBackward0]
	1961974049472 -> 1961974048656
	1961974049472 [label=ReluBackward0]
	1961974048176 -> 1961974049472
	1961974048176 [label=AddBackward0]
	1961974048080 -> 1961974048176
	1961974048080 [label=NativeBatchNormBackward0]
	1961974047936 -> 1961974048080
	1961974047936 [label=ConvolutionBackward0]
	1961974047552 -> 1961974047936
	1961974047552 [label=ReluBackward0]
	1961974047408 -> 1961974047552
	1961974047408 [label=NativeBatchNormBackward0]
	1961974047216 -> 1961974047408
	1961974047216 [label=ConvolutionBackward0]
	1961974048128 -> 1961974047216
	1961974048128 [label=ReluBackward0]
	1961974046880 -> 1961974048128
	1961974046880 [label=AddBackward0]
	1961974046784 -> 1961974046880
	1961974046784 [label=NativeBatchNormBackward0]
	1961971527392 -> 1961974046784
	1961971527392 [label=ConvolutionBackward0]
	1961971527104 -> 1961971527392
	1961971527104 [label=ReluBackward0]
	1961971526864 -> 1961971527104
	1961971526864 [label=NativeBatchNormBackward0]
	1961971526672 -> 1961971526864
	1961971526672 [label=ConvolutionBackward0]
	1961974047072 -> 1961971526672
	1961974047072 [label=ReluBackward0]
	1961971526384 -> 1961974047072
	1961971526384 [label=AddBackward0]
	1961971526288 -> 1961971526384
	1961971526288 [label=NativeBatchNormBackward0]
	1961971525952 -> 1961971526288
	1961971525952 [label=ConvolutionBackward0]
	1961971525664 -> 1961971525952
	1961971525664 [label=ReluBackward0]
	1961971525424 -> 1961971525664
	1961971525424 [label=NativeBatchNormBackward0]
	1961971525328 -> 1961971525424
	1961971525328 [label=ConvolutionBackward0]
	1961971526336 -> 1961971525328
	1961971526336 [label=ReluBackward0]
	1961971525040 -> 1961971526336
	1961971525040 [label=AddBackward0]
	1961971524848 -> 1961971525040
	1961971524848 [label=NativeBatchNormBackward0]
	1961971524608 -> 1961971524848
	1961971524608 [label=ConvolutionBackward0]
	1961971524416 -> 1961971524608
	1961971524416 [label=ReluBackward0]
	1961971524272 -> 1961971524416
	1961971524272 [label=NativeBatchNormBackward0]
	1961971524080 -> 1961971524272
	1961971524080 [label=ConvolutionBackward0]
	1961971524896 -> 1961971524080
	1961971524896 [label=ReluBackward0]
	1961971523744 -> 1961971524896
	1961971523744 [label=AddBackward0]
	1961971523648 -> 1961971523744
	1961971523648 [label=NativeBatchNormBackward0]
	1961971494624 -> 1961971523648
	1961971494624 [label=ConvolutionBackward0]
	1961971494336 -> 1961971494624
	1961971494336 [label=ReluBackward0]
	1961971494192 -> 1961971494336
	1961971494192 [label=NativeBatchNormBackward0]
	1961971494096 -> 1961971494192
	1961971494096 [label=ConvolutionBackward0]
	1961971493808 -> 1961971494096
	1961971493808 [label=ReluBackward0]
	1961971493568 -> 1961971493808
	1961971493568 [label=AddBackward0]
	1961971493472 -> 1961971493568
	1961971493472 [label=NativeBatchNormBackward0]
	1961971493328 -> 1961971493472
	1961971493328 [label=ConvolutionBackward0]
	1961971493040 -> 1961971493328
	1961971493040 [label=ReluBackward0]
	1961971492800 -> 1961971493040
	1961971492800 [label=NativeBatchNormBackward0]
	1961971492608 -> 1961971492800
	1961971492608 [label=ConvolutionBackward0]
	1961971493520 -> 1961971492608
	1961971493520 [label=ReluBackward0]
	1961971492224 -> 1961971493520
	1961971492224 [label=AddBackward0]
	1961971492128 -> 1961971492224
	1961971492128 [label=NativeBatchNormBackward0]
	1961971491984 -> 1961971492128
	1961971491984 [label=ConvolutionBackward0]
	1961971491600 -> 1961971491984
	1961971491600 [label=ReluBackward0]
	1961971491360 -> 1961971491600
	1961971491360 [label=NativeBatchNormBackward0]
	1961971491120 -> 1961971491360
	1961971491120 [label=ConvolutionBackward0]
	1961971492176 -> 1961971491120
	1961971492176 [label=ReluBackward0]
	1961971490880 -> 1961971492176
	1961971490880 [label=AddBackward0]
	1961971491024 -> 1961971490880
	1961971491024 [label=NativeBatchNormBackward0]
	1961971461808 -> 1961971491024
	1961971461808 [label=ConvolutionBackward0]
	1961971461520 -> 1961971461808
	1961971461520 [label=ReluBackward0]
	1961971461184 -> 1961971461520
	1961971461184 [label=NativeBatchNormBackward0]
	1961971461088 -> 1961971461184
	1961971461088 [label=ConvolutionBackward0]
	1961971462048 -> 1961971461088
	1961971462048 [label=ReluBackward0]
	1961971460800 -> 1961971462048
	1961971460800 [label=AddBackward0]
	1961971460608 -> 1961971460800
	1961971460608 [label=NativeBatchNormBackward0]
	1961971460368 -> 1961971460608
	1961971460368 [label=ConvolutionBackward0]
	1961971460080 -> 1961971460368
	1961971460080 [label=ReluBackward0]
	1961971459840 -> 1961971460080
	1961971459840 [label=NativeBatchNormBackward0]
	1961971459744 -> 1961971459840
	1961971459744 [label=ConvolutionBackward0]
	1961971459552 -> 1961971459744
	1961971459552 [label=ReluBackward0]
	1961971459312 -> 1961971459552
	1961971459312 [label=AddBackward0]
	1961971459120 -> 1961971459312
	1961971459120 [label=NativeBatchNormBackward0]
	1961971458880 -> 1961971459120
	1961971458880 [label=ConvolutionBackward0]
	1961971458592 -> 1961971458880
	1961971458592 [label=ReluBackward0]
	1961971458448 -> 1961971458592
	1961971458448 [label=NativeBatchNormBackward0]
	1961971458352 -> 1961971458448
	1961971458352 [label=ConvolutionBackward0]
	1961971459168 -> 1961971458352
	1961971459168 [label=ReluBackward0]
	1961971433232 -> 1961971459168
	1961971433232 [label=AddBackward0]
	1961971433136 -> 1961971433232
	1961971433136 [label=NativeBatchNormBackward0]
	1961971432800 -> 1961971433136
	1961971432800 [label=ConvolutionBackward0]
	1961971432608 -> 1961971432800
	1961971432608 [label=ReluBackward0]
	1961971432464 -> 1961971432608
	1961971432464 [label=NativeBatchNormBackward0]
	1961971432368 -> 1961971432464
	1961971432368 [label=ConvolutionBackward0]
	1961971433184 -> 1961971432368
	1961971433184 [label=ReluBackward0]
	1961971431888 -> 1961971433184
	1961971431888 [label=AddBackward0]
	1961971431696 -> 1961971431888
	1961971431696 [label=NativeBatchNormBackward0]
	1961971431456 -> 1961971431696
	1961971431456 [label=ConvolutionBackward0]
	1961971431264 -> 1961971431456
	1961971431264 [label=ReluBackward0]
	1961971431120 -> 1961971431264
	1961971431120 [label=NativeBatchNormBackward0]
	1961971430928 -> 1961971431120
	1961971430928 [label=ConvolutionBackward0]
	1961971431744 -> 1961971430928
	1961971431744 [label=MaxPool2DWithIndicesBackward0]
	1961971430448 -> 1961971431744
	1961971430448 [label=ReluBackward0]
	1961971430256 -> 1961971430448
	1961971430256 [label=NativeBatchNormBackward0]
	1961971430160 -> 1961971430256
	1961971430160 [label=ConvolutionBackward0]
	1961971429968 -> 1961971430160
	1961874534864 [label="model.encoder.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1961874534864 -> 1961971429968
	1961971429968 [label=AccumulateGrad]
	1961971430208 -> 1961971430256
	1961874530192 [label="model.encoder.bn1.weight
 (64)" fillcolor=lightblue]
	1961874530192 -> 1961971430208
	1961971430208 [label=AccumulateGrad]
	1961971430736 -> 1961971430256
	1961874530032 [label="model.encoder.bn1.bias
 (64)" fillcolor=lightblue]
	1961874530032 -> 1961971430736
	1961971430736 [label=AccumulateGrad]
	1961971430640 -> 1961971430928
	1961874529152 [label="model.encoder.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1961874529152 -> 1961971430640
	1961971430640 [label=AccumulateGrad]
	1961971430976 -> 1961971431120
	1961874528672 [label="model.encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1961874528672 -> 1961971430976
	1961971430976 [label=AccumulateGrad]
	1961971431216 -> 1961971431120
	1961874528432 [label="model.encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1961874528432 -> 1961971431216
	1961971431216 [label=AccumulateGrad]
	1961971431312 -> 1961971431456
	1961874527792 [label="model.encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1961874527792 -> 1961971431312
	1961971431312 [label=AccumulateGrad]
	1961971431504 -> 1961971431696
	1961874527392 [label="model.encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1961874527392 -> 1961971431504
	1961971431504 [label=AccumulateGrad]
	1961971431552 -> 1961971431696
	1961874527152 [label="model.encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1961874527152 -> 1961971431552
	1961971431552 [label=AccumulateGrad]
	1961971431744 -> 1961971431888
	1961971431984 -> 1961971432368
	1961874526512 [label="model.encoder.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1961874526512 -> 1961971431984
	1961971431984 [label=AccumulateGrad]
	1961971432416 -> 1961971432464
	1961874525936 [label="model.encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1961874525936 -> 1961971432416
	1961971432416 [label=AccumulateGrad]
	1961971432560 -> 1961971432464
	1961874525696 [label="model.encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1961874525696 -> 1961971432560
	1961971432560 [label=AccumulateGrad]
	1961971432656 -> 1961971432800
	1961874524976 [label="model.encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1961874524976 -> 1961971432656
	1961971432656 [label=AccumulateGrad]
	1961971432944 -> 1961971433136
	1961874524576 [label="model.encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1961874524576 -> 1961971432944
	1961971432944 [label=AccumulateGrad]
	1961971432992 -> 1961971433136
	1961874524336 [label="model.encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1961874524336 -> 1961971432992
	1961971432992 [label=AccumulateGrad]
	1961971433184 -> 1961971433232
	1961971458112 -> 1961971458352
	1961874523696 [label="model.encoder.layer1.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1961874523696 -> 1961971458112
	1961971458112 [label=AccumulateGrad]
	1961971458400 -> 1961971458448
	1961874523216 [label="model.encoder.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1961874523216 -> 1961971458400
	1961971458400 [label=AccumulateGrad]
	1961971458544 -> 1961971458448
	1961874523056 [label="model.encoder.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1961874523056 -> 1961971458544
	1961971458544 [label=AccumulateGrad]
	1961971458640 -> 1961971458880
	1961874522336 [label="model.encoder.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1961874522336 -> 1961971458640
	1961971458640 [label=AccumulateGrad]
	1961971459024 -> 1961971459120
	1961874517744 [label="model.encoder.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1961874517744 -> 1961971459024
	1961971459024 [label=AccumulateGrad]
	1961971459072 -> 1961971459120
	1961874517504 [label="model.encoder.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1961874517504 -> 1961971459072
	1961971459072 [label=AccumulateGrad]
	1961971459168 -> 1961971459312
	1961971459600 -> 1961971459744
	1961874516864 [label="model.encoder.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1961874516864 -> 1961971459600
	1961971459600 [label=AccumulateGrad]
	1961971459792 -> 1961971459840
	1961874516384 [label="model.encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1961874516384 -> 1961971459792
	1961971459792 [label=AccumulateGrad]
	1961971459936 -> 1961971459840
	1961874516224 [label="model.encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1961874516224 -> 1961971459936
	1961971459936 [label=AccumulateGrad]
	1961971460128 -> 1961971460368
	1961874515504 [label="model.encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961874515504 -> 1961971460128
	1961971460128 [label=AccumulateGrad]
	1961971460416 -> 1961971460608
	1961874515104 [label="model.encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1961874515104 -> 1961971460416
	1961971460416 [label=AccumulateGrad]
	1961971460560 -> 1961971460608
	1961874514864 [label="model.encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1961874514864 -> 1961971460560
	1961971460560 [label=AccumulateGrad]
	1961971460752 -> 1961971460800
	1961971460752 [label=NativeBatchNormBackward0]
	1961971459648 -> 1961971460752
	1961971459648 [label=ConvolutionBackward0]
	1961971459552 -> 1961971459648
	1961971459504 -> 1961971459648
	1961874514224 [label="model.encoder.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1961874514224 -> 1961971459504
	1961971459504 [label=AccumulateGrad]
	1961971460272 -> 1961971460752
	1961874513648 [label="model.encoder.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1961874513648 -> 1961971460272
	1961971460272 [label=AccumulateGrad]
	1961971460320 -> 1961971460752
	1961874513408 [label="model.encoder.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1961874513408 -> 1961971460320
	1961971460320 [label=AccumulateGrad]
	1961971460896 -> 1961971461088
	1961874512768 [label="model.encoder.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961874512768 -> 1961971460896
	1961971460896 [label=AccumulateGrad]
	1961971461136 -> 1961971461184
	1961874512288 [label="model.encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1961874512288 -> 1961971461136
	1961971461136 [label=AccumulateGrad]
	1961971461376 -> 1961971461184
	1961874512048 [label="model.encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1961874512048 -> 1961971461376
	1961971461376 [label=AccumulateGrad]
	1961971461568 -> 1961971461808
	1961874511408 [label="model.encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961874511408 -> 1961971461568
	1961971461568 [label=AccumulateGrad]
	1961971461856 -> 1961971491024
	1961874511008 [label="model.encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1961874511008 -> 1961971461856
	1961971461856 [label=AccumulateGrad]
	1961971462000 -> 1961971491024
	1961874510768 [label="model.encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1961874510768 -> 1961971462000
	1961971462000 [label=AccumulateGrad]
	1961971462048 -> 1961971490880
	1961971490928 -> 1961971491120
	1961874510128 [label="model.encoder.layer2.2.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961874510128 -> 1961971490928
	1961971490928 [label=AccumulateGrad]
	1961971491312 -> 1961971491360
	1961874509552 [label="model.encoder.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1961874509552 -> 1961971491312
	1961971491312 [label=AccumulateGrad]
	1961971491552 -> 1961971491360
	1961874509312 [label="model.encoder.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1961874509312 -> 1961971491552
	1961971491552 [label=AccumulateGrad]
	1961971491648 -> 1961971491984
	1961874508672 [label="model.encoder.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961874508672 -> 1961971491648
	1961971491648 [label=AccumulateGrad]
	1961971492032 -> 1961971492128
	1961874508272 [label="model.encoder.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1961874508272 -> 1961971492032
	1961971492032 [label=AccumulateGrad]
	1961971492080 -> 1961971492128
	1961874508032 [label="model.encoder.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1961874508032 -> 1961971492080
	1961971492080 [label=AccumulateGrad]
	1961971492176 -> 1961971492224
	1961971492320 -> 1961971492608
	1961874507392 [label="model.encoder.layer2.3.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961874507392 -> 1961971492320
	1961971492320 [label=AccumulateGrad]
	1961971492752 -> 1961971492800
	1961874506912 [label="model.encoder.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1961874506912 -> 1961971492752
	1961971492752 [label=AccumulateGrad]
	1961971492896 -> 1961971492800
	1961874506672 [label="model.encoder.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1961874506672 -> 1961971492896
	1961971492896 [label=AccumulateGrad]
	1961971493088 -> 1961971493328
	1961874506032 [label="model.encoder.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961874506032 -> 1961971493088
	1961971493088 [label=AccumulateGrad]
	1961971493376 -> 1961971493472
	1961874484336 [label="model.encoder.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1961874484336 -> 1961971493376
	1961971493376 [label=AccumulateGrad]
	1961971493424 -> 1961971493472
	1961874481536 [label="model.encoder.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1961874481536 -> 1961971493424
	1961971493424 [label=AccumulateGrad]
	1961971493520 -> 1961971493568
	1961971493856 -> 1961971494096
	1961874483216 [label="model.encoder.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1961874483216 -> 1961971493856
	1961971493856 [label=AccumulateGrad]
	1961971494144 -> 1961971494192
	1961874484016 [label="model.encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1961874484016 -> 1961971494144
	1961971494144 [label=AccumulateGrad]
	1961971494288 -> 1961971494192
	1961874483296 [label="model.encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1961874483296 -> 1961971494288
	1961971494288 [label=AccumulateGrad]
	1961971494384 -> 1961971494624
	1961874481856 [label="model.encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961874481856 -> 1961971494384
	1961971494384 [label=AccumulateGrad]
	1961971494768 -> 1961971523648
	1961874484416 [label="model.encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1961874484416 -> 1961971494768
	1961971494768 [label=AccumulateGrad]
	1961971494816 -> 1961971523648
	1961874483936 [label="model.encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1961874483936 -> 1961971494816
	1961971494816 [label=AccumulateGrad]
	1961971523936 -> 1961971523744
	1961971523936 [label=NativeBatchNormBackward0]
	1961971494000 -> 1961971523936
	1961971494000 [label=ConvolutionBackward0]
	1961971493808 -> 1961971494000
	1961971493664 -> 1961971494000
	1961874483056 [label="model.encoder.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1961874483056 -> 1961971493664
	1961971493664 [label=AccumulateGrad]
	1961971494432 -> 1961971523936
	1961874482256 [label="model.encoder.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1961874482256 -> 1961971494432
	1961971494432 [label=AccumulateGrad]
	1961971494576 -> 1961971523936
	1961874482016 [label="model.encoder.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1961874482016 -> 1961971494576
	1961971494576 [label=AccumulateGrad]
	1961971523840 -> 1961971524080
	1961874485472 [label="model.encoder.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961874485472 -> 1961971523840
	1961971523840 [label=AccumulateGrad]
	1961971524128 -> 1961971524272
	1961874486272 [label="model.encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1961874486272 -> 1961971524128
	1961971524128 [label=AccumulateGrad]
	1961971524368 -> 1961971524272
	1961874488112 [label="model.encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1961874488112 -> 1961971524368
	1961971524368 [label=AccumulateGrad]
	1961971524464 -> 1961971524608
	1961874487952 [label="model.encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961874487952 -> 1961971524464
	1961971524464 [label=AccumulateGrad]
	1961971524656 -> 1961971524848
	1961874487312 [label="model.encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1961874487312 -> 1961971524656
	1961971524656 [label=AccumulateGrad]
	1961971524704 -> 1961971524848
	1961874486992 [label="model.encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1961874486992 -> 1961971524704
	1961971524704 [label=AccumulateGrad]
	1961971524896 -> 1961971525040
	1961971525136 -> 1961971525328
	1961874485712 [label="model.encoder.layer3.2.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961874485712 -> 1961971525136
	1961971525136 [label=AccumulateGrad]
	1961971525376 -> 1961971525424
	1961874488992 [label="model.encoder.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1961874488992 -> 1961971525376
	1961971525376 [label=AccumulateGrad]
	1961971525616 -> 1961971525424
	1961874395024 [label="model.encoder.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1961874395024 -> 1961971525616
	1961971525616 [label=AccumulateGrad]
	1961971525808 -> 1961971525952
	1961874393904 [label="model.encoder.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961874393904 -> 1961971525808
	1961971525808 [label=AccumulateGrad]
	1961971526096 -> 1961971526288
	1961874394464 [label="model.encoder.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1961874394464 -> 1961971526096
	1961971526096 [label=AccumulateGrad]
	1961971526144 -> 1961971526288
	1961874394864 [label="model.encoder.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1961874394864 -> 1961971526144
	1961971526144 [label=AccumulateGrad]
	1961971526336 -> 1961971526384
	1961971526480 -> 1961971526672
	1961874393264 [label="model.encoder.layer3.3.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961874393264 -> 1961971526480
	1961971526480 [label=AccumulateGrad]
	1961971526720 -> 1961971526864
	1961874392784 [label="model.encoder.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1961874392784 -> 1961971526720
	1961971526720 [label=AccumulateGrad]
	1961971527056 -> 1961971526864
	1961874392304 [label="model.encoder.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1961874392304 -> 1961971527056
	1961971527056 [label=AccumulateGrad]
	1961971527152 -> 1961971527392
	1961874545136 [label="model.encoder.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961874545136 -> 1961971527152
	1961971527152 [label=AccumulateGrad]
	1961971527536 -> 1961974046784
	1961874544576 [label="model.encoder.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1961874544576 -> 1961971527536
	1961971527536 [label=AccumulateGrad]
	1961971527584 -> 1961974046784
	1961874544256 [label="model.encoder.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1961874544256 -> 1961971527584
	1961971527584 [label=AccumulateGrad]
	1961974047072 -> 1961974046880
	1961974046976 -> 1961974047216
	1961874543136 [label="model.encoder.layer3.4.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961874543136 -> 1961974046976
	1961974046976 [label=AccumulateGrad]
	1961974047264 -> 1961974047408
	1961874545536 [label="model.encoder.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1961874545536 -> 1961974047264
	1961974047264 [label=AccumulateGrad]
	1961974047504 -> 1961974047408
	1961874545776 [label="model.encoder.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1961874545776 -> 1961974047504
	1961974047504 [label=AccumulateGrad]
	1961974047696 -> 1961974047936
	1961951076416 [label="model.encoder.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961951076416 -> 1961974047696
	1961974047696 [label=AccumulateGrad]
	1961974047984 -> 1961974048080
	1961951076816 [label="model.encoder.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1961951076816 -> 1961974047984
	1961974047984 [label=AccumulateGrad]
	1961974048032 -> 1961974048080
	1961951077056 [label="model.encoder.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1961951077056 -> 1961974048032
	1961974048032 [label=AccumulateGrad]
	1961974048128 -> 1961974048176
	1961974048272 -> 1961974048656
	1961951077856 [label="model.encoder.layer3.5.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961951077856 -> 1961974048272
	1961974048272 [label=AccumulateGrad]
	1961974048704 -> 1961974048752
	1961951078256 [label="model.encoder.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1961951078256 -> 1961974048704
	1961974048704 [label=AccumulateGrad]
	1961974048944 -> 1961974048752
	1961951078496 [label="model.encoder.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1961951078496 -> 1961974048944
	1961974048944 [label=AccumulateGrad]
	1961974049136 -> 1961974049280
	1961951079296 [label="model.encoder.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1961951079296 -> 1961974049136
	1961974049136 [label=AccumulateGrad]
	1961974049328 -> 1961974049424
	1961951079696 [label="model.encoder.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1961951079696 -> 1961974049328
	1961974049328 [label=AccumulateGrad]
	1961974049376 -> 1961974049424
	1961951079936 [label="model.encoder.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1961951079936 -> 1961974049376
	1961974049376 [label=AccumulateGrad]
	1961974049472 -> 1961974049520
	1961974049904 -> 1961974050048
	1961951134080 [label="model.encoder.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1961951134080 -> 1961974049904
	1961974049904 [label=AccumulateGrad]
	1961974050192 -> 1961974050240
	1961951134480 [label="model.encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1961951134480 -> 1961974050192
	1961974050192 [label=AccumulateGrad]
	1961974050432 -> 1961974050240
	1961951134720 [label="model.encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1961951134720 -> 1961974050432
	1961974050432 [label=AccumulateGrad]
	1961974050528 -> 1961974050672
	1961951135520 [label="model.encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1961951135520 -> 1961974050528
	1961974050528 [label=AccumulateGrad]
	1961974050720 -> 1961974075456
	1961951135920 [label="model.encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1961951135920 -> 1961974050720
	1961974050720 [label=AccumulateGrad]
	1961974050768 -> 1961974075456
	1961951136160 [label="model.encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1961951136160 -> 1961974050768
	1961974050768 [label=AccumulateGrad]
	1961974075600 -> 1961974075648
	1961974075600 [label=NativeBatchNormBackward0]
	1961974049952 -> 1961974075600
	1961974049952 [label=ConvolutionBackward0]
	1961974049760 -> 1961974049952
	1961974049712 -> 1961974049952
	1961951136960 [label="model.encoder.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1961951136960 -> 1961974049712
	1961974049712 [label=AccumulateGrad]
	1961974050576 -> 1961974075600
	1961951137360 [label="model.encoder.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1961951137360 -> 1961974050576
	1961974050576 [label=AccumulateGrad]
	1961974050624 -> 1961974075600
	1961951137600 [label="model.encoder.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1961951137600 -> 1961974050624
	1961974050624 [label=AccumulateGrad]
	1961974075840 -> 1961974076128
	1961951208128 [label="model.encoder.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1961951208128 -> 1961974075840
	1961974075840 [label=AccumulateGrad]
	1961974076272 -> 1961974076320
	1961951208528 [label="model.encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1961951208528 -> 1961974076272
	1961974076272 [label=AccumulateGrad]
	1961974076416 -> 1961974076320
	1961951208768 [label="model.encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1961951208768 -> 1961974076416
	1961974076416 [label=AccumulateGrad]
	1961974076512 -> 1961974076656
	1961951209568 [label="model.encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1961951209568 -> 1961974076512
	1961974076512 [label=AccumulateGrad]
	1961974076704 -> 1961974076896
	1961951209968 [label="model.encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1961951209968 -> 1961974076704
	1961974076704 [label=AccumulateGrad]
	1961974076848 -> 1961974076896
	1961951210208 [label="model.encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1961951210208 -> 1961974076848
	1961974076848 [label=AccumulateGrad]
	1961974077040 -> 1961974077088
	1961974077184 -> 1961974077568
	1961951211008 [label="model.encoder.layer4.2.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1961951211008 -> 1961974077184
	1961974077184 [label=AccumulateGrad]
	1961974077616 -> 1961974077760
	1961951211408 [label="model.encoder.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1961951211408 -> 1961974077616
	1961974077616 [label=AccumulateGrad]
	1961974077712 -> 1961974077760
	1961951277280 [label="model.encoder.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1961951277280 -> 1961974077712
	1961974077712 [label=AccumulateGrad]
	1961974077904 -> 1961974110864
	1961951278080 [label="model.encoder.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1961951278080 -> 1961974077904
	1961974077904 [label=AccumulateGrad]
	1961974078672 -> 1961974108704
	1961951278480 [label="model.encoder.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1961951278480 -> 1961974078672
	1961974078672 [label=AccumulateGrad]
	1961974078624 -> 1961974108704
	1961951278720 [label="model.encoder.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1961951278720 -> 1961974078624
	1961974078624 [label=AccumulateGrad]
	1961974111872 -> 1961974110768
	1961974112016 -> 1961974111728
	1961951279520 [label="model.decoder.p5.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1961951279520 -> 1961974112016
	1961974112016 [label=AccumulateGrad]
	1961974111680 -> 1961974111728
	1961951279840 [label="model.decoder.p5.bias
 (256)" fillcolor=lightblue]
	1961951279840 -> 1961974111680
	1961974111680 [label=AccumulateGrad]
	1961974110912 -> 1961974111056
	1961965823376 [label="model.decoder.seg_blocks.0.block.0.block.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	1961965823376 -> 1961974110912
	1961974110912 [label=AccumulateGrad]
	1961974111104 -> 1961845643008
	1961965823776 [label="model.decoder.seg_blocks.0.block.0.block.1.weight
 (128)" fillcolor=lightblue]
	1961965823776 -> 1961974111104
	1961974111104 [label=AccumulateGrad]
	1961974111968 -> 1961845643008
	1961965824016 [label="model.decoder.seg_blocks.0.block.0.block.1.bias
 (128)" fillcolor=lightblue]
	1961965824016 -> 1961974111968
	1961974111968 [label=AccumulateGrad]
	1961845643344 -> 1961845643488
	1961965824336 [label="model.decoder.seg_blocks.0.block.1.block.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961965824336 -> 1961845643344
	1961845643344 [label=AccumulateGrad]
	1961845643440 -> 1961845643392
	1961965824736 [label="model.decoder.seg_blocks.0.block.1.block.1.weight
 (128)" fillcolor=lightblue]
	1961965824736 -> 1961845643440
	1961845643440 [label=AccumulateGrad]
	1961845643056 -> 1961845643392
	1961965824976 [label="model.decoder.seg_blocks.0.block.1.block.1.bias
 (128)" fillcolor=lightblue]
	1961965824976 -> 1961845643056
	1961845643056 [label=AccumulateGrad]
	1961845644016 -> 1961845643824
	1961965825296 [label="model.decoder.seg_blocks.0.block.2.block.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961965825296 -> 1961845644016
	1961845644016 [label=AccumulateGrad]
	1961845642864 -> 1961845644112
	1961965825696 [label="model.decoder.seg_blocks.0.block.2.block.1.weight
 (128)" fillcolor=lightblue]
	1961965825696 -> 1961845642864
	1961845642864 [label=AccumulateGrad]
	1961845644928 -> 1961845644112
	1961965825936 [label="model.decoder.seg_blocks.0.block.2.block.1.bias
 (128)" fillcolor=lightblue]
	1961965825936 -> 1961845644928
	1961845644928 [label=AccumulateGrad]
	1961845645024 -> 1961845643248
	1961845645024 [label=UpsampleBilinear2DBackward0]
	1961845644160 -> 1961845645024
	1961845644160 [label=ReluBackward0]
	1961845643920 -> 1961845644160
	1961845643920 [label=NativeGroupNormBackward0]
	1961845643776 -> 1961845643920
	1961845643776 [label=ConvolutionBackward0]
	1961845644544 -> 1961845643776
	1961845644544 [label=UpsampleBilinear2DBackward0]
	1961974110816 -> 1961845644544
	1961974110816 [label=ReluBackward0]
	1961974112064 -> 1961974110816
	1961974112064 [label=NativeGroupNormBackward0]
	1961974111824 -> 1961974112064
	1961974111824 [label=ConvolutionBackward0]
	1961974076608 -> 1961974111824
	1961974076608 [label=AddBackward0]
	1961974076560 -> 1961974076608
	1961974076560 [label=UpsampleNearest2DBackward0]
	1961974111728 -> 1961974076560
	1961974077136 -> 1961974076608
	1961974077136 [label=ConvolutionBackward0]
	1961974049760 -> 1961974077136
	1961974075888 -> 1961974077136
	1961951280240 [label="model.decoder.p4.skip_conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	1961951280240 -> 1961974075888
	1961974075888 [label=AccumulateGrad]
	1961974076080 -> 1961974077136
	1961951280560 [label="model.decoder.p4.skip_conv.bias
 (256)" fillcolor=lightblue]
	1961951280560 -> 1961974076080
	1961974076080 [label=AccumulateGrad]
	1961974077664 -> 1961974111824
	1961965961440 [label="model.decoder.seg_blocks.1.block.0.block.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	1961965961440 -> 1961974077664
	1961974077664 [label=AccumulateGrad]
	1961974077952 -> 1961974112064
	1961965961840 [label="model.decoder.seg_blocks.1.block.0.block.1.weight
 (128)" fillcolor=lightblue]
	1961965961840 -> 1961974077952
	1961974077952 [label=AccumulateGrad]
	1961974078528 -> 1961974112064
	1961965962080 [label="model.decoder.seg_blocks.1.block.0.block.1.bias
 (128)" fillcolor=lightblue]
	1961965962080 -> 1961974078528
	1961974078528 [label=AccumulateGrad]
	1961845643296 -> 1961845643776
	1961965962400 [label="model.decoder.seg_blocks.1.block.1.block.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1961965962400 -> 1961845643296
	1961845643296 [label=AccumulateGrad]
	1961845643536 -> 1961845643920
	1961965962800 [label="model.decoder.seg_blocks.1.block.1.block.1.weight
 (128)" fillcolor=lightblue]
	1961965962800 -> 1961845643536
	1961845643536 [label=AccumulateGrad]
	1961845644736 -> 1961845643920
	1961965963040 [label="model.decoder.seg_blocks.1.block.1.block.1.bias
 (128)" fillcolor=lightblue]
	1961965963040 -> 1961845644736
	1961845644736 [label=AccumulateGrad]
	1961845642816 -> 1961845644832
	1961845642816 [label=UpsampleBilinear2DBackward0]
	1961845643872 -> 1961845642816
	1961845643872 [label=ReluBackward0]
	1961845643968 -> 1961845643872
	1961845643968 [label=NativeGroupNormBackward0]
	1961974110624 -> 1961845643968
	1961974110624 [label=ConvolutionBackward0]
	1961974076368 -> 1961974110624
	1961974076368 [label=AddBackward0]
	1961974075792 -> 1961974076368
	1961974075792 [label=UpsampleNearest2DBackward0]
	1961974076608 -> 1961974075792
	1961974077376 -> 1961974076368
	1961974077376 [label=ConvolutionBackward0]
	1961971493808 -> 1961974077376
	1961974049232 -> 1961974077376
	1961951280960 [label="model.decoder.p3.skip_conv.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1961951280960 -> 1961974049232
	1961974049232 [label=AccumulateGrad]
	1961974049184 -> 1961974077376
	1961965822176 [label="model.decoder.p3.skip_conv.bias
 (256)" fillcolor=lightblue]
	1961965822176 -> 1961974049184
	1961974049184 [label=AccumulateGrad]
	1961974075936 -> 1961974110624
	1961965963360 [label="model.decoder.seg_blocks.2.block.0.block.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	1961965963360 -> 1961974075936
	1961974075936 [label=AccumulateGrad]
	1961974108752 -> 1961845643968
	1961965963760 [label="model.decoder.seg_blocks.2.block.0.block.1.weight
 (128)" fillcolor=lightblue]
	1961965963760 -> 1961974108752
	1961974108752 [label=AccumulateGrad]
	1961974111008 -> 1961845643968
	1961965964000 [label="model.decoder.seg_blocks.2.block.0.block.1.bias
 (128)" fillcolor=lightblue]
	1961965964000 -> 1961974111008
	1961974111008 [label=AccumulateGrad]
	1961845645264 -> 1962223844080
	1961845645264 [label=ReluBackward0]
	1961845644880 -> 1961845645264
	1961845644880 [label=NativeGroupNormBackward0]
	1961974077520 -> 1961845644880
	1961974077520 [label=ConvolutionBackward0]
	1961974048320 -> 1961974077520
	1961974048320 [label=AddBackward0]
	1961974047888 -> 1961974048320
	1961974047888 [label=UpsampleNearest2DBackward0]
	1961974076368 -> 1961974047888
	1961974048800 -> 1961974048320
	1961974048800 [label=ConvolutionBackward0]
	1961971459552 -> 1961974048800
	1961974048464 -> 1961974048800
	1961965822576 [label="model.decoder.p2.skip_conv.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1961965822576 -> 1961974048464
	1961974048464 [label=AccumulateGrad]
	1961974048224 -> 1961974048800
	1961965822896 [label="model.decoder.p2.skip_conv.bias
 (256)" fillcolor=lightblue]
	1961965822896 -> 1961974048224
	1961974048224 [label=AccumulateGrad]
	1961974050000 -> 1961974077520
	1961965964320 [label="model.decoder.seg_blocks.3.block.0.block.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	1961965964320 -> 1961974050000
	1961974050000 [label=AccumulateGrad]
	1961974077328 -> 1961845644880
	1961965964720 [label="model.decoder.seg_blocks.3.block.0.block.1.weight
 (128)" fillcolor=lightblue]
	1961965964720 -> 1961974077328
	1961974077328 [label=AccumulateGrad]
	1961845644784 -> 1961845644880
	1961965964960 [label="model.decoder.seg_blocks.3.block.0.block.1.bias
 (128)" fillcolor=lightblue]
	1961965964960 -> 1961845644784
	1961845644784 [label=AccumulateGrad]
	1961845644496 -> 1962223844944
	1961969848544 [label="model.segmentation_head.0.weight
 (4, 128, 1, 1)" fillcolor=lightblue]
	1961969848544 -> 1961845644496
	1961845644496 [label=AccumulateGrad]
	1961845645120 -> 1962223844944
	1961969848864 [label="model.segmentation_head.0.bias
 (4)" fillcolor=lightblue]
	1961969848864 -> 1961845645120
	1961845645120 [label=AccumulateGrad]
	1962223845232 -> 1961225538352
}
